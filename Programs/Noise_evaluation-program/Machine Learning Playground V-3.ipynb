{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Detection Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data presented are measurements of a gaussian beam for varying beam-frequencies and distances. Due to technical difficulties, our measuring device would sometimes crash and provide us with completely noisy data, or data that was only half complete. Our intent was to automize the measuring process, by making the lab-computer automatically evaluate the data. In the case of faulty data, it was supposed to restart the measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path # to extract data from registry\n",
    "import pandas as pd # data frames\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable #adjust colorbars to axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "# spli data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# process data\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# classification algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# model evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=Path('.')\n",
    "# list(path.glob'./*.dat') finds all data \".dat\" data in entered directory\n",
    "paths=list([x for x in p.iterdir() if x.is_dir() and x.name=='Measurements'][0].glob('./*.dat')) #use ** to also include subregistries\n",
    "#remember to change name if directory name is changed\n",
    "# generate lists\n",
    "path_names=list(map(lambda x:x.name, paths))\n",
    "data_dict={path.name: np.genfromtxt(path,skip_header=1)[:,2] for path in paths}\n",
    "# add images\n",
    "size=np.int(np.sqrt(len(data_dict[path_names[0]])))\n",
    "data_dict[\"images\"]=[data_dict[name].reshape((size,size)) for name in path_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets\n",
    "\n",
    "# key targets 0=noise, 1=okay data, 2= good data\n",
    "three_targets={'map00Ghz-Rauschmessung-Gleb.dat':0,'map100GHz-2-Gleb.dat':2\n",
    "               ,'map100GHz-Gleb.dat':0,'map105GHz-2-Gleb.dat':2,'map105GHz-Gleb.dat':0\n",
    "               ,'map110GHz-19_06.-Gleb.dat':0,'map110GHz-test1-Gleb.dat':0\n",
    "               ,'map70GHz-test1-Gleb.dat':2,'map70GHz-Tu16-Nadine.dat':0\n",
    "               ,'map75GHz-Mo15-Nadine.dat':2,'map80GHz-Mo15-Nadine.dat':2\n",
    "               ,'map85GHz-2-Gleb.dat':0,'map85GHz-3-Gleb.dat':0\n",
    "               ,'map85GHz-4-Gleb.dat':0,'map85GHz-5-Gleb.dat':0\n",
    "               ,'map85GHz-80cm-19_06.-Gleb.dat':2,'map85GHz-Gleb.dat':1\n",
    "               ,'map85GHz-test6-Gleb.dat':1,'map85GHz-test7-Gleb.dat':2\n",
    "               ,'map90GHz-2-Gleb.dat':2,'map90GHz-80cm-19_06.-Gleb.dat':2\n",
    "               ,'map90GHz-Gleb.dat':1,'map95GHz-19_06.-Gleb.dat':0\n",
    "               ,'map95GHz-2-19_06.-Gleb.dat':2,'map95GHz-2-Gleb.dat':2\n",
    "               ,'map95GHz-80cm-19_06.-Gleb.dat':2,'map95GHz-Gleb.dat':2\n",
    "               ,'map95GHz-Mo15-Nadine.dat':2,'map95GHz-nadine.dat':1\n",
    "               ,'map95GHz-nadine2.dat':2,'map95GHz-test2-Gleb.dat':0\n",
    "               ,'map95GHz-test3-Gleb.dat':0,'map95GHz-Tu16-Nadine.dat':0\n",
    "               ,'map80GHz-1-120cm-23_06.-Gleb.dat':2,'map85GHz-1-120cm-23_06.-Gleb.dat':2\n",
    "               ,'map85GHz-2-120cm-23_06.-Gleb.dat':2,'map85GHz-3-120cm-23_06.-Gleb.dat':2\n",
    "               ,'ma70GHz-1-120cm-24_06.-Nadine.dat':2,'ma75GHz-1-120cm-24_06.-Nadine.dat':2\n",
    "               ,'map100GHz-1-120cm-24_06.-Nadine.dat':2,'map105GHz-2-120cm-24_06.-Nadine.dat':2\n",
    "               ,'map110GHz-1-120cm-24_06.-Nadine.dat':0,'map90GHz-1-120cm-23_06.-Gleb.dat':2\n",
    "               ,'map95GHz-1-120cm-23_06.-Gleb.dat':2,'Rauschdaten_120mm.dat':0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming Dictionary out of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.zeros(len(three_targets)).astype(int)\n",
    "X=np.zeros((len(three_targets),len(data_dict[path_names[0]])))\n",
    "size=np.int(np.sqrt(len(data_dict[path_names[0]])))\n",
    "names=len(three_targets)*[\"\"]\n",
    "# counter\n",
    "count=0\n",
    "# format data correctly\n",
    "for name in path_names:\n",
    "    if name in three_targets:\n",
    "        names[count]=name\n",
    "        y[count]=three_targets[name]\n",
    "        X[count,:]=data_dict[name]\n",
    "        count+=1\n",
    "    else:\n",
    "        print(\"{} not yet labeled\".format(name))\n",
    "beam_data={\"data_names\":names,\"data\":X,\"target\":y,\n",
    "           \"target_names\":[\"full noise\",\"half_noise\",\"no noise\"],\"images\":[x.reshape((size,size)) for x in X]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#all data\n",
    "rows=int(len(path_names) / 3) + (len(path_names) % 3 > 0) # how many rows\n",
    "##############################\n",
    "rows=1\n",
    "##################################\n",
    "# plot\n",
    "fig = plt.figure(figsize=(18, 5*rows))\n",
    "for i,name in enumerate([path_names[i] for i in [17,18,37]]):\n",
    "    ax=plt.subplot(rows,3,i+1)\n",
    "    ax.set_axis_off() # hide axis\n",
    "    im=ax.imshow(data_dict[\"images\"][list(data_dict.keys()).index(name)],cmap='jet', interpolation='nearest')\n",
    "    if name in beam_data[\"data_names\"]:\n",
    "        ax.set_title(\"name: {} \\n labeled: {}\".format(name,beam_data[\"target_names\"][beam_data[\"target\"][list(beam_data[\"data_names\"]).index(name)]]),fontsize=14)\n",
    "    else:\n",
    "        ax.set_title(\"name: {}\\n not labeled yet\".format(name),color=\"red\")\n",
    "        print(\"'{}'\".format(name))\n",
    "    #adjust colorbar to plot\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"questionable-labels.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_local_max(X):\n",
    "    return [X[i]/maxi for i,maxi in enumerate(np.amax(X,axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_local_max(X):\n",
    "    return [X[i]/(maxi-mini) for i,(maxi,mini) in enumerate(zip(np.amax(X,axis=1),np.amin(X,axis=1)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rotating for symmetrical prediction (generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the transition to half_noise always occurs from no_noise in the top region to full_noise in the bottom region. To generalize the algorithm for other problems, the half data will now be rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For even further generalization, all data in 2D representation can be repositioned to have their peak in the center. This way, dispositions of the curve location would not need to be considered within the model, which could improve not only generalization performace, but also general performance in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale\n",
    "X_total=rescale_local_max(beam_data[\"data\"])\n",
    "# take half data\n",
    "X_half_noise=np.array(X_total)[np.where(beam_data[\"target\"]==1)]\n",
    "# X_data in total\n",
    "X_added=np.zeros((X_half_noise.shape[0]*3,X_half_noise.shape[1]))\n",
    "for i in range(X_half_noise.shape[0]):\n",
    "        X_added[3*i,:]=np.rot90(X_half_noise[i].reshape((size,size))).flatten()\n",
    "        X_added[3*i+1,:]=np.rot90(X_added[3*i,:].reshape((size,size))).flatten()\n",
    "        X_added[3*i+2,:]=np.rot90(X_added[3*i+1,:].reshape((size,size))).flatten()\n",
    "X_total=np.vstack((X_total,X_added))\n",
    "y_total=np.concatenate((beam_data[\"target\"],[1]*X_half_noise.shape[0]*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide example of rotated data\n",
    "fig=plt.figure(figsize=(10,2.5))\n",
    "ax=plt.subplot(1,4,1)\n",
    "ax.imshow(X_half_noise[3,:].reshape((size,size)),cmap=\"jet\")\n",
    "ax.set_title(\"original\")\n",
    "ax.set_axis_off() # hide axis\n",
    "for i in range(3):\n",
    "    ax=plt.subplot(1,4,i+2)\n",
    "    ax.imshow(X_added[9+i,:].reshape((size,size)),cmap=\"jet\")\n",
    "    ax.set_title(\"rotated by {}°\".format(90*(i+1)))\n",
    "    ax.set_axis_off() # hide axis\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rotated-maps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total,y_total, stratify=y_total, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (applying PCA + knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this application we have few samples (40) and many features (441). To reduce the number of features, we apply Prinicipal Component Analysis (PCA), to reduce the dimension in feature space (Here from 221 to 2) and therefore improve the performance of our alogorithm. We use the k-neighbours classifier (knn), as it performs particulary well on small sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and it's data rescaling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale to mean=0, std=1\n",
    "scaler = StandardScaler()\n",
    "# fit scaling\n",
    "scaler.fit(X_train)\n",
    "# apply scaling\n",
    "scaled_X_train=scaler.transform(X_train)\n",
    "# n_components=amount of principal components\n",
    "pca = PCA(n_components=2) # n_components=0.95 alternatively\n",
    "# fit PCA model to beast cancer data\n",
    "pca.fit(scaled_X_train)\n",
    "# transform\n",
    "pca_X_train = pca.transform(scaled_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "# PCA on train data\n",
    "g0 = pca_X_train[np.where(y_train==0)]\n",
    "g1 = pca_X_train[np.where(y_train==1)]\n",
    "g2 = pca_X_train[np.where(y_train==2)]\n",
    "\n",
    "train_data = (g0, g1, g2)\n",
    "colors = (\"red\",\"orange\",\"blue\")\n",
    "groups = (\"full_noise\", \"half_noise\",\"no_noise\")\n",
    "train_marker=(\"o\")\n",
    "# PCA on test data\n",
    "# transform\n",
    "scaled_X_test=scaler.transform(X_test)\n",
    "pca_X_test = pca.transform(scaled_X_test)\n",
    "# # # # #\n",
    "h0 = pca_X_test[np.where(y_test==0)]\n",
    "h1 = pca_X_test[np.where(y_test==1)]\n",
    "h2 = pca_X_test[np.where(y_test==2)]\n",
    "test_data = (h0, h1, h2)\n",
    "test_marker=(\"^\")\n",
    "# Create plot\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(1, 1, 1, )\n",
    "# plot train-transform\n",
    "for data, color, group in zip(train_data, colors, groups):\n",
    "    x =data[:,0]\n",
    "    y =data[:,1]\n",
    "    ax.scatter(x, y, alpha=0.7, c=color, edgecolors='black', s=50, label=\"train:\"+group, marker=train_marker)\n",
    "# plot test-transform\n",
    "for data, color, group in zip(test_data, colors, groups):\n",
    "    x =data[:,0]\n",
    "    y =data[:,1]\n",
    "    ax.scatter(x, y, alpha=0.7, c=color, edgecolors='black', s=100, label=\"test:\"+group, marker=test_marker)\n",
    "    # labels\n",
    "#plt.title('PCA-transformed plot')\n",
    "plt.xlabel(\"prinicipal component Nr.1\",fontsize=15)\n",
    "plt.ylabel(\"principal component Nr.2\",fontsize=15)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PCA_map.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot components\n",
    "fig, axes = plt.subplots(1, 2,figsize=(15,6))\n",
    "for i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n",
    "    im=ax.imshow(component.reshape((size,size)),cmap='jet',interpolation=\"nearest\")\n",
    "    ax.set_title(\"component Nr.{}\".format(i+1),fontsize=30)\n",
    "    ax.set_xlabel(\"Pixel in x-direction\",fontsize=25)\n",
    "    ax.set_ylabel(\"Pixel in y-direction\",fontsize=25)\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "#fig.suptitle('PCA component weights', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PCA-componenets.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and fitting pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because of small sample-size, no parameter optimization will be conducted \n",
    "(as splitting the data in another validation set would reduce the already small test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"component_analyzer\", PCA(n_components=2)),\n",
    "                 (\"classifier\", KNeighborsClassifier(n_neighbors=1))])# fitting\n",
    "pipe.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General evaluation via stratified KFold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stratified makes sure that all classes are represented in each training set. Shuffle makes sure, that the data is shuffled before it is split and is only necessary when the data is sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "print(\"Cross-validation scores:\\n{}\".format(\n",
    "      cross_val_score(pipe, X_total, y_total, cv=kfold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix shows, how test samples were classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross calidation confusion matrix\n",
    "for i,split in enumerate(splits):\n",
    "    pipe.fit(X_total[split[0]],y_total[split[0]])\n",
    "    print(\"Split Nr.{}:\\n{}\\n\".format(i,confusion_matrix(pipe.predict(X_total[split[1]]),y_total[split[1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict(X_total[splits[4][1]])[pipe.predict(X_total[splits[4][1]])!=y_total[splits[4][1]]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_data[\"target_names\"][pipe.predict(X_total[splits[4][1]])[pipe.predict(X_total[splits[4][1]])!=y_total[splits[4][1]]][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam that was misclassified\n",
    "index=np.where(pipe.predict(X_total[splits[4][1]])!=y_total[splits[4][1]])\n",
    "plt.figure(figsize=(5,5.5))\n",
    "ax=plt.subplot(1,1,1)\n",
    "ax.set_axis_off() # hide axis\n",
    "ax.set_title(\"target= {}\\npredicted= {}\".format(beam_data[\"target_names\"][y_total[index][0]],\n",
    "            beam_data[\"target_names\"][pipe.predict(X_total[splits[4][1]])[pipe.predict(X_total[splits[4][1]])!=y_total[splits[4][1]]][0]]),\n",
    "            fontsize=25)\n",
    "ax.imshow(X_total[index].reshape((size,size)), \n",
    "          cmap=\"jet\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"false-prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of one singular split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"confusion matrix:\\n{}\".format(confusion_matrix(y_test,predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### falsely classified in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect indices\n",
    "index=[]\n",
    "predictions=pipe.predict(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i]!=predictions[i]:\n",
    "        print(y_test[i],predictions[i])\n",
    "        index.append(i)\n",
    "    \n",
    "# show false predictions in test_data\n",
    "rows=int(len(index) / 3) + (len(index) % 3 > 0) # how many rows\n",
    "# plot\n",
    "fig = plt.figure(figsize=(20, 5*rows+1))\n",
    "for i,ind in enumerate(index):\n",
    "    ax=plt.subplot(rows,3,i+1)\n",
    "    im=ax.imshow(X_test[ind].reshape((size,size)),cmap='jet', interpolation='nearest')\n",
    "    ax.set_title(\"pred: {},\\n correct: {}\".format(beam_data[\"target_names\"][predictions[ind]],\n",
    "                                             beam_data[\"target_names\"][y_test[ind]]))\n",
    "    #adjust colorbar to plot\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    plt.colorbar(im, cax=cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### falsely classified in train data (unreasonable here, due to knn:n_neighbours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect indices\n",
    "index=[]\n",
    "predictions=pipe.predict(X_train)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]!=predictions[i]:\n",
    "        print((y_train[i],predictions[i]))\n",
    "        index.append(i)\n",
    "        \n",
    "# show false predictions in train data\n",
    "rows=int(len(index) / 3) + (len(index) % 3 > 0) # how many rows\n",
    "# plot\n",
    "fig = plt.figure(figsize=(20, 5*rows+1))\n",
    "for i,ind in enumerate(index):\n",
    "    ax=plt.subplot(rows,3,i+1)\n",
    "    im=ax.imshow(X_train[ind].reshape((size,size)),cmap='jet', interpolation='nearest')\n",
    "    ax.set_title(\"pred: {},\\n correct: {}\".format(beam_data[\"target_names\"][predictions[ind]],\n",
    "                                             beam_data[\"target_names\"][y_train[ind]]))\n",
    "    #adjust colorbar to plot\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    plt.colorbar(im, cax=cax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
